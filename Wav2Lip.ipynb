{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wav2Lip_quick_trial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gagandeepreehal/ML-DL-RL-Notebooks/blob/main/Wav2Lip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSQFs_G8caeE"
      },
      "source": [
        "# Collab preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIVB0Xn1g6ih",
        "outputId": "33002711-743d-4e14-80e4-95fa35cfda67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qciH4PsUazL_",
        "outputId": "a42bfe29-04dc-43cb-e3b1-24e0fae6e216",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ5taGmPcWV-"
      },
      "source": [
        "# Get the code and models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3LihClHbUd3",
        "outputId": "376eb5d5-33d3-49f9-ef35-7600f8d42aad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!git clone https://github.com/Rudrabha/Wav2Lip.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Wav2Lip'...\n",
            "remote: Enumerating objects: 57, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/57)\u001b[K\rremote: Counting objects:   3% (2/57)\u001b[K\rremote: Counting objects:   5% (3/57)\u001b[K\rremote: Counting objects:   7% (4/57)\u001b[K\rremote: Counting objects:   8% (5/57)\u001b[K\rremote: Counting objects:  10% (6/57)\u001b[K\rremote: Counting objects:  12% (7/57)\u001b[K\rremote: Counting objects:  14% (8/57)\u001b[K\rremote: Counting objects:  15% (9/57)\u001b[K\rremote: Counting objects:  17% (10/57)\u001b[K\rremote: Counting objects:  19% (11/57)\u001b[K\rremote: Counting objects:  21% (12/57)\u001b[K\rremote: Counting objects:  22% (13/57)\u001b[K\rremote: Counting objects:  24% (14/57)\u001b[K\rremote: Counting objects:  26% (15/57)\u001b[K\rremote: Counting objects:  28% (16/57)\u001b[K\rremote: Counting objects:  29% (17/57)\u001b[K\rremote: Counting objects:  31% (18/57)\u001b[K\rremote: Counting objects:  33% (19/57)\u001b[K\rremote: Counting objects:  35% (20/57)\u001b[K\rremote: Counting objects:  36% (21/57)\u001b[K\rremote: Counting objects:  38% (22/57)\u001b[K\rremote: Counting objects:  40% (23/57)\u001b[K\rremote: Counting objects:  42% (24/57)\u001b[K\rremote: Counting objects:  43% (25/57)\u001b[K\rremote: Counting objects:  45% (26/57)\u001b[K\rremote: Counting objects:  47% (27/57)\u001b[K\rremote: Counting objects:  49% (28/57)\u001b[K\rremote: Counting objects:  50% (29/57)\u001b[K\rremote: Counting objects:  52% (30/57)\u001b[K\rremote: Counting objects:  54% (31/57)\u001b[K\rremote: Counting objects:  56% (32/57)\u001b[K\rremote: Counting objects:  57% (33/57)\u001b[K\rremote: Counting objects:  59% (34/57)\u001b[K\rremote: Counting objects:  61% (35/57)\u001b[K\rremote: Counting objects:  63% (36/57)\u001b[K\rremote: Counting objects:  64% (37/57)\u001b[K\rremote: Counting objects:  66% (38/57)\u001b[K\rremote: Counting objects:  68% (39/57)\u001b[K\rremote: Counting objects:  70% (40/57)\u001b[K\rremote: Counting objects:  71% (41/57)\u001b[K\rremote: Counting objects:  73% (42/57)\u001b[K\rremote: Counting objects:  75% (43/57)\u001b[K\rremote: Counting objects:  77% (44/57)\u001b[K\rremote: Counting objects:  78% (45/57)\u001b[K\rremote: Counting objects:  80% (46/57)\u001b[K\rremote: Counting objects:  82% (47/57)\u001b[K\rremote: Counting objects:  84% (48/57)\u001b[K\rremote: Counting objects:  85% (49/57)\u001b[K\rremote: Counting objects:  87% (50/57)\u001b[K\rremote: Counting objects:  89% (51/57)\u001b[K\rremote: Counting objects:  91% (52/57)\u001b[K\rremote: Counting objects:  92% (53/57)\u001b[K\rremote: Counting objects:  94% (54/57)\u001b[K\rremote: Counting objects:  96% (55/57)\u001b[K\rremote: Counting objects:  98% (56/57)\u001b[K\rremote: Counting objects: 100% (57/57)\u001b[K\rremote: Counting objects: 100% (57/57), done.\u001b[K\n",
            "remote: Compressing objects:   2% (1/47)\u001b[K\rremote: Compressing objects:   4% (2/47)\u001b[K\rremote: Compressing objects:   6% (3/47)\u001b[K\rremote: Compressing objects:   8% (4/47)\u001b[K\rremote: Compressing objects:  10% (5/47)\u001b[K\rremote: Compressing objects:  12% (6/47)\u001b[K\rremote: Compressing objects:  14% (7/47)\u001b[K\rremote: Compressing objects:  17% (8/47)\u001b[K\rremote: Compressing objects:  19% (9/47)\u001b[K\rremote: Compressing objects:  21% (10/47)\u001b[K\rremote: Compressing objects:  23% (11/47)\u001b[K\rremote: Compressing objects:  25% (12/47)\u001b[K\rremote: Compressing objects:  27% (13/47)\u001b[K\rremote: Compressing objects:  29% (14/47)\u001b[K\rremote: Compressing objects:  31% (15/47)\u001b[K\rremote: Compressing objects:  34% (16/47)\u001b[K\rremote: Compressing objects:  36% (17/47)\u001b[K\rremote: Compressing objects:  38% (18/47)\u001b[K\rremote: Compressing objects:  40% (19/47)\u001b[K\rremote: Compressing objects:  42% (20/47)\u001b[K\rremote: Compressing objects:  44% (21/47)\u001b[K\rremote: Compressing objects:  46% (22/47)\u001b[K\rremote: Compressing objects:  48% (23/47)\u001b[K\rremote: Compressing objects:  51% (24/47)\u001b[K\rremote: Compressing objects:  53% (25/47)\u001b[K\rremote: Compressing objects:  55% (26/47)\u001b[K\rremote: Compressing objects:  57% (27/47)\u001b[K\rremote: Compressing objects:  59% (28/47)\u001b[K\rremote: Compressing objects:  61% (29/47)\u001b[K\rremote: Compressing objects:  63% (30/47)\u001b[K\rremote: Compressing objects:  65% (31/47)\u001b[K\rremote: Compressing objects:  68% (32/47)\u001b[K\rremote: Compressing objects:  70% (33/47)\u001b[K\rremote: Compressing objects:  72% (34/47)\u001b[K\rremote: Compressing objects:  74% (35/47)\u001b[K\rremote: Compressing objects:  76% (36/47)\u001b[K\rremote: Compressing objects:  78% (37/47)\u001b[K\rremote: Compressing objects:  80% (38/47)\u001b[K\rremote: Compressing objects:  82% (39/47)\u001b[K\rremote: Compressing objects:  85% (40/47)\u001b[K\rremote: Compressing objects:  87% (41/47)\u001b[K\rremote: Compressing objects:  89% (42/47)\u001b[K\rremote: Compressing objects:  91% (43/47)\u001b[K\rremote: Compressing objects:  93% (44/47)\u001b[K\rremote: Compressing objects:  95% (45/47)\u001b[K\rremote: Compressing objects:  97% (46/47)\u001b[K\rremote: Compressing objects: 100% (47/47)\u001b[K\rremote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "Unpacking objects:   1% (1/57)   \rUnpacking objects:   3% (2/57)   \rUnpacking objects:   5% (3/57)   \rUnpacking objects:   7% (4/57)   \rUnpacking objects:   8% (5/57)   \rUnpacking objects:  10% (6/57)   \rUnpacking objects:  12% (7/57)   \rUnpacking objects:  14% (8/57)   \rUnpacking objects:  15% (9/57)   \rUnpacking objects:  17% (10/57)   \rUnpacking objects:  19% (11/57)   \rUnpacking objects:  21% (12/57)   \rUnpacking objects:  22% (13/57)   \rUnpacking objects:  24% (14/57)   \rUnpacking objects:  26% (15/57)   \rUnpacking objects:  28% (16/57)   \rUnpacking objects:  29% (17/57)   \rUnpacking objects:  31% (18/57)   \rUnpacking objects:  33% (19/57)   \rUnpacking objects:  35% (20/57)   \rUnpacking objects:  36% (21/57)   \rUnpacking objects:  38% (22/57)   \rUnpacking objects:  40% (23/57)   \rUnpacking objects:  42% (24/57)   \rUnpacking objects:  43% (25/57)   \rUnpacking objects:  45% (26/57)   \rUnpacking objects:  47% (27/57)   \rUnpacking objects:  49% (28/57)   \rUnpacking objects:  50% (29/57)   \rUnpacking objects:  52% (30/57)   \rUnpacking objects:  54% (31/57)   \rUnpacking objects:  56% (32/57)   \rUnpacking objects:  57% (33/57)   \rUnpacking objects:  59% (34/57)   \rUnpacking objects:  61% (35/57)   \rUnpacking objects:  63% (36/57)   \rUnpacking objects:  64% (37/57)   \rUnpacking objects:  66% (38/57)   \rremote: Total 57 (delta 7), reused 46 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects:  68% (39/57)   \rUnpacking objects:  70% (40/57)   \rUnpacking objects:  71% (41/57)   \rUnpacking objects:  73% (42/57)   \rUnpacking objects:  75% (43/57)   \rUnpacking objects:  77% (44/57)   \rUnpacking objects:  78% (45/57)   \rUnpacking objects:  80% (46/57)   \rUnpacking objects:  82% (47/57)   \rUnpacking objects:  84% (48/57)   \rUnpacking objects:  85% (49/57)   \rUnpacking objects:  87% (50/57)   \rUnpacking objects:  89% (51/57)   \rUnpacking objects:  91% (52/57)   \rUnpacking objects:  92% (53/57)   \rUnpacking objects:  94% (54/57)   \rUnpacking objects:  96% (55/57)   \rUnpacking objects:  98% (56/57)   \rUnpacking objects: 100% (57/57)   \rUnpacking objects: 100% (57/57), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-19nzx8SamJ",
        "outputId": "f403d610-9eb7-45f8-95c8-0cd5b053565d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  Wav2Lip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjzMPy_Sb0AI"
      },
      "source": [
        "!cp -ri \"/content/gdrive/My Drive/Wav2lip/wav2lip_gan.pth\" /content/Wav2Lip/checkpoints/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWTaOS3ncFt6"
      },
      "source": [
        "# Get the pre-requisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ooh28vw-Uvd3"
      },
      "source": [
        "!pip uninstall tensorflow tensorflow-gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49dCYlLdcK2D"
      },
      "source": [
        "!cd Wav2Lip && pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ey_bN4M6X_95",
        "outputId": "40ea1940-5630-481f-8ce3-118908999d8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" -O \"Wav2Lip/face_detection/detection/sfd/s3fd.pth\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-24 16:50:30--  https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\n",
            "Resolving www.adrianbulat.com (www.adrianbulat.com)... 95.179.192.69\n",
            "Connecting to www.adrianbulat.com (www.adrianbulat.com)|95.179.192.69|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 89843225 (86M) [application/octet-stream]\n",
            "Saving to: ‘Wav2Lip/face_detection/detection/sfd/s3fd.pth’\n",
            "\n",
            "Wav2Lip/face_detect 100%[===================>]  85.68M   499KB/s    in 2m 55s  \n",
            "\n",
            "2020-08-24 16:53:25 (502 KB/s) - ‘Wav2Lip/face_detection/detection/sfd/s3fd.pth’ saved [89843225/89843225]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdIQfY2Kswcb"
      },
      "source": [
        "# Now lets try!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoVGMtjRZfeR",
        "outputId": "a65ca468-7beb-45f7-c174-724c239d4475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!cp \"/content/gdrive/My Drive/Wav2Lip/input_vid.mp4\" \"/content/gdrive/My Drive/Wav2Lip/input_audio.wav\" sample_data/\n",
        "!ls sample_data/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "anscombe.json\t\t      input_audio.wav  mnist_train_small.csv\n",
            "california_housing_test.csv   input_vid.mp4    README.md\n",
            "california_housing_train.csv  mnist_test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR5utmDMcSZY",
        "outputId": "3b71b71d-dbe0-4469-e48d-b7221a1d6564",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!cd Wav2Lip && python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face \"../sample_data/input_vid.mp4\" --audio \"../sample_data/input_audio.wav\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda for inference.\n",
            "Reading video frames...\n",
            "Number of frames available for inference: 185\n",
            "(80, 593)\n",
            "Length of mel chunks: 181\n",
            "  0% 0/2 [00:00<?, ?it/s]\n",
            "  0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 1/12 [00:16<03:01, 16.46s/it]\u001b[A\n",
            " 17% 2/12 [00:17<01:58, 11.88s/it]\u001b[A\n",
            " 25% 3/12 [00:18<01:18,  8.69s/it]\u001b[A\n",
            " 33% 4/12 [00:20<00:51,  6.43s/it]\u001b[A\n",
            " 42% 5/12 [00:21<00:33,  4.85s/it]\u001b[A\n",
            " 50% 6/12 [00:22<00:22,  3.76s/it]\u001b[A\n",
            " 58% 7/12 [00:23<00:14,  2.99s/it]\u001b[A\n",
            " 67% 8/12 [00:24<00:09,  2.46s/it]\u001b[A\n",
            " 75% 9/12 [00:26<00:06,  2.08s/it]\u001b[A\n",
            " 83% 10/12 [00:27<00:03,  1.81s/it]\u001b[A\n",
            " 92% 11/12 [00:28<00:01,  1.62s/it]\u001b[A\n",
            "100% 12/12 [00:36<00:00,  3.07s/it]\n",
            "Load checkpoint from: checkpoints/wav2lip_gan.pth\n",
            "Model loaded\n",
            "100% 2/2 [00:49<00:00, 24.83s/it]\n",
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "\u001b[0;33mGuessed Channel Layout for Input Stream #0.0 : stereo\n",
            "\u001b[0mInput #0, wav, from '../sample_data/input_audio.wav':\n",
            "  Metadata:\n",
            "    encoder         : Lavf58.29.100\n",
            "  Duration: 00:00:07.40, bitrate: 1536 kb/s\n",
            "    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, stereo, s16, 1536 kb/s\n",
            "Input #1, avi, from 'temp/result.avi':\n",
            "  Metadata:\n",
            "    encoder         : Lavf58.26.101\n",
            "  Duration: 00:00:07.24, start: 0.000000, bitrate: 937 kb/s\n",
            "    Stream #1:0: Video: mpeg4 (Simple Profile) (DIVX / 0x58564944), yuv420p, 600x480 [SAR 1:1 DAR 5:4], 931 kb/s, 25 fps, 25 tbr, 25 tbn, 25 tbc\n",
            "Stream mapping:\n",
            "  Stream #1:0 -> #0:0 (mpeg4 (native) -> h264 (libx264))\n",
            "  Stream #0:0 -> #0:1 (pcm_s16le (native) -> aac (native))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x55f21018a800] \u001b[0m\u001b[0;33m-qscale is ignored, -crf is recommended.\n",
            "\u001b[0m\u001b[1;36m[libx264 @ 0x55f21018a800] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x55f21018a800] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
            "\u001b[1;36m[libx264 @ 0x55f21018a800] \u001b[0mprofile High, level 3.0\n",
            "\u001b[1;36m[libx264 @ 0x55f21018a800] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'results/result_voice.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p(progressive), 600x480 [SAR 1:1 DAR 5:4], q=-1--1, 25 fps, 12800 tbn, 25 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "    Stream #0:1: Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 128 kb/s\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 aac\n",
            "frame=  181 fps= 61 q=-1.0 Lsize=     466kB time=00:00:07.40 bitrate= 515.4kbits/s speed= 2.5x    \n",
            "video:340kB audio:120kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 1.346917%\n",
            "\u001b[1;36m[libx264 @ 0x55f21018a800] \u001b[0mframe I:1     Avg QP:17.11  size:  9276\n",
            "\u001b[1;36m[libx264 @ 0x55f21018a800] \u001b[0mframe P:119   Avg QP:19.32  size:  2433\n",
            "\u001b[1;36m[libx264 @ 0x55f21018a800] \u001b[0mframe B:61    Avg QP:22.73  size:   795\n",
            "\u001b[1;36m[libx264 @ 0x55f21018a800] \u001b[0mconsecutive B-frames: 48.6% 15.5% 11.6% 24.3%\n",
            "\u001b[1;36m[libx264 @ 0x55f21018a800] \u001b[0mmb I  I16..4: 50.4% 43.9%  5.7%\n",
            "\u001b[1;36m[libx264 @ 0x55f21018a800] \u001b[0mmb P  I16..4:  1.9%  4.1%  0.2%  P16..4: 26.0%  7.5%  3.4%  0.0%  0.0%    skip:57.0%\n",
            "\u001b[1;36m[libx264 @ 0x55f21018a800] \u001b[0mmb B  I16..4:  0.5%  1.6%  0.0%  B16..8: 26.2%  2.6%  0.2%  direct: 0.3%  skip:68.5%  L0:47.8% L1:46.3% BI: 5.9%\n",
            "\u001b[1;36m[libx264 @ 0x55f21018a800] \u001b[0m8x8 transform intra:65.6% inter:76.0%\n",
            "\u001b[1;36m[libx264 @ 0x55f21018a800] \u001b[0mcoded y,uvDC,uvAC intra: 32.1% 46.4% 4.1% inter: 6.9% 7.3% 0.0%\n",
            "\u001b[1;36m[libx264 @ 0x55f21018a800] \u001b[0mi16 v,h,dc,p: 32% 34% 19% 14%\n",
            "\u001b[1;36m[libx264 @ 0x55f21018a800] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 25% 23% 42%  2%  1%  2%  1%  1%  2%\n",
            "\u001b[1;36m[libx264 @ 0x55f21018a800] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 30% 23% 16%  3%  7%  9%  5%  5%  2%\n",
            "\u001b[1;36m[libx264 @ 0x55f21018a800] \u001b[0mi8c dc,h,v,p: 46% 26% 25%  3%\n",
            "\u001b[1;36m[libx264 @ 0x55f21018a800] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
            "\u001b[1;36m[libx264 @ 0x55f21018a800] \u001b[0mref P L0: 73.1% 11.3% 11.4%  4.1%\n",
            "\u001b[1;36m[libx264 @ 0x55f21018a800] \u001b[0mref B L0: 86.8% 10.8%  2.3%\n",
            "\u001b[1;36m[libx264 @ 0x55f21018a800] \u001b[0mref B L1: 97.3%  2.7%\n",
            "\u001b[1;36m[libx264 @ 0x55f21018a800] \u001b[0mkb/s:383.74\n",
            "\u001b[1;36m[aac @ 0x55f21025e000] \u001b[0mQavg: 574.082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNOAZvkszEOw"
      },
      "source": [
        "# use the \"files\" button on the left to download the result in the Wav2Lip/results/ folder."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7zgfrQqbKom"
      },
      "source": [
        "## **Variations to try**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f9A9VDVbZAG"
      },
      "source": [
        "1.   Use more padding to include the chin region"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45XW4SZAzIz5"
      },
      "source": [
        "!cd Wav2Lip && python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face \"../sample_data/input_vid.mp4\" --audio \"../sample_data/input_audio.wav\" --pads 0 20 0 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo-WnsxfbwTG"
      },
      "source": [
        "2.   Use resize_factor to reduce the video resolution, as there is a change you might get better results for lower resolution videos. Why? Because the model was trained on low resolution faces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw0xFtZ2bsx8"
      },
      "source": [
        "!cd Wav2Lip && python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face \"../sample_data/input_vid.mp4\" --audio \"../sample_data/input_audio.wav\" --resize_factor 2"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}